{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install sklearn for Windows devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install sklearn for MacOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U scikit-learn\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>00017</th>\n",
       "      <th>0003</th>\n",
       "      <th>0004</th>\n",
       "      <th>0005</th>\n",
       "      <th>001</th>\n",
       "      <th>0021</th>\n",
       "      <th>003</th>\n",
       "      <th>...</th>\n",
       "      <th>zwf1</th>\n",
       "      <th>zwitterionic</th>\n",
       "      <th>zygomycete</th>\n",
       "      <th>zygomycetes</th>\n",
       "      <th>zygotic</th>\n",
       "      <th>zymogen</th>\n",
       "      <th>zymogens</th>\n",
       "      <th>zymogram</th>\n",
       "      <th>zymography</th>\n",
       "      <th>zymomonas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0001  00017  0003  0004  0005  001  0021  003  ...  zwf1  \\\n",
       "0   0    0     0      0     0     0     0    0     0    0  ...     0   \n",
       "1   0    0     0      0     0     0     0    0     0    0  ...     0   \n",
       "2   0    0     0      0     0     0     0    0     0    0  ...     0   \n",
       "3   0    0     0      0     0     0     0    0     0    0  ...     0   \n",
       "4   0    0     0      0     0     0     0    0     0    0  ...     0   \n",
       "\n",
       "   zwitterionic  zygomycete  zygomycetes  zygotic  zymogen  zymogens  \\\n",
       "0             0           0            0        0        0         0   \n",
       "1             0           0            0        0        0         0   \n",
       "2             0           0            0        0        0         0   \n",
       "3             0           0            0        0        0         0   \n",
       "4             0           0            0        0        0         0   \n",
       "\n",
       "   zymogram  zymography  zymomonas  \n",
       "0         0           0          0  \n",
       "1         0           0          0  \n",
       "2         0           0          0  \n",
       "3         0           0          0  \n",
       "4         0           0          0  \n",
       "\n",
       "[5 rows x 26179 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = pd.read_csv('trg.csv')\n",
    "vectorizer = CountVectorizer()\n",
    "matrix = vectorizer.fit_transform(data.abstract)\n",
    "\n",
    "data = data.rename(columns={'class': 'class_label'})\n",
    "X = data.drop('class_label', axis=1)\n",
    "y = data['class_label']\n",
    "data  = pd.DataFrame(data= matrix.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "5-fold cross-validation list of accuracies: [0.96, 0.9875, 0.9875, 0.99375, 0.99625]\n",
      "5-fold cross-validation mean accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['abstract'], y, test_size=0.1, random_state=42)\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.word_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.class_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.class_priors = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.vocab = set()\n",
    "        for cls in self.classes:\n",
    "            self.class_counts[cls] = sum(1 for label in y if label == cls)\n",
    "        for abstract, label in zip(X, y):\n",
    "            for word in set(abstract.split()):\n",
    "                self.word_counts[label][word] += 1\n",
    "                self.vocab.add(word)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        total_docs = len(X)\n",
    "        self.class_priors = {cls: self.class_counts[cls] / total_docs for cls in self.classes}\n",
    "\n",
    "    def predict(self, documents):\n",
    "        predictions = []\n",
    "        for document in documents:\n",
    "            posterior_probs = {cls: np.log(self.class_priors[cls]) for cls in self.classes}\n",
    "            document_words = document.split()\n",
    "            word_counts = {word: document_words.count(word) for word in set(document_words)}\n",
    "            for word, count in word_counts.items():\n",
    "                for cls in self.classes:\n",
    "                    word_count_in_class = self.word_counts[cls].get(word, 0)\n",
    "                    total_words_in_class = sum(self.word_counts[cls].values())\n",
    "                    conditional_prob = (word_count_in_class + 1) / (total_words_in_class + self.vocab_size)\n",
    "                    posterior_probs[cls] += count * np.log(conditional_prob)\n",
    "            predicted_class = max(posterior_probs, key=posterior_probs.get)\n",
    "            predictions.append(predicted_class)\n",
    "        return predictions\n",
    "    \n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "accuracies = []\n",
    "\n",
    "nb_clf = NaiveBayes()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "y_pred = nb_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "for train_index, test_index in kf.split(X['abstract']):\n",
    "    X_train, X_test = X['abstract'][train_index], X['abstract'][test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    nb_clf.fit(X_train, y_train)\n",
    "    y_pred = nb_clf.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(\"5-fold cross-validation list of accuracies:\",accuracies)\n",
    "print(f\"5-fold cross-validation mean accuracy: {np.mean(accuracies):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Naive Bayes model with extra-preprocessing (stopwords, use of n-grams, stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1000, 4000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ENGLISH_STOP_WORDS\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[1;32m----> 9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNaiveBayes\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, stemming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2657\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2661\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2662\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <3x1 sparse matrix ...>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1000, 4000]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['abstract'], y, test_size=0.1, random_state=42)\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, alpha=1.0, stemming=False):\n",
    "        self.word_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.class_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.class_priors = {}\n",
    "        self.alpha = alpha\n",
    "        self.stemming = stemming\n",
    "        if self.stemming:\n",
    "            self.stemmer = PorterStemmer()\n",
    "            \n",
    "            \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"alpha\": self.alpha,\n",
    "            \"stemming\": self.stemming,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        words = text.split()\n",
    "        processed_words = []\n",
    "        skip_next = False\n",
    "        skip_next_next = False\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if skip_next:\n",
    "                skip_next = False\n",
    "                if skip_next_next:\n",
    "                    skip_next = True\n",
    "                    skip_next_next = False\n",
    "                continue\n",
    "\n",
    "            if word == \"homo\" and i + 1 < len(words) and words[i + 1] == \"sapiens\":\n",
    "                processed_words.append(\"homo-sapiens\")\n",
    "                skip_next = True\n",
    "                continue\n",
    "\n",
    "            if word == \"escherichia\" and i + 1 < len(words) and words[i + 1] == \"coli\":\n",
    "                processed_words.append(\"escherichia-coli\")\n",
    "                skip_next = True\n",
    "                continue\n",
    "\n",
    "            if word == \"human\" and i + 2 < len(words) and words[i + 1] == \"immunodeficiency\" and words[i + 2] == \"virus\":\n",
    "                processed_words.append(\"human-immunodeficiency-virus\")\n",
    "                skip_next = True\n",
    "                skip_next_next = True\n",
    "                continue\n",
    "\n",
    "            if self.stemming:\n",
    "                word = self.stemmer.stem(word)\n",
    "\n",
    "            if word not in ENGLISH_STOP_WORDS:\n",
    "                processed_words.append(word)\n",
    "\n",
    "        return \" \".join(processed_words)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.vocab = set()\n",
    "\n",
    "        for cls in self.classes:\n",
    "            self.class_counts[cls] = sum(1 for label in y if label == cls)\n",
    "\n",
    "        for abstract, label in zip(X, y):\n",
    "            abstract = self.preprocess(abstract)\n",
    "            for word in set(abstract.split()):\n",
    "                self.word_counts[label][word] += 1\n",
    "                self.vocab.add(word)\n",
    "\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        total_docs = len(X)\n",
    "        self.class_priors = {cls: self.class_counts[cls] / total_docs for cls in self.classes}\n",
    "\n",
    "    def predict(self, documents):\n",
    "        predictions = []\n",
    "        for document in documents:\n",
    "            document = self.preprocess(document)\n",
    "            posterior_probs = {cls: np.log(self.class_priors[cls]) for cls in self.classes}\n",
    "            document_words = document.split()\n",
    "            word_counts = {word: document_words.count(word) for word in set(document_words)}\n",
    "\n",
    "            for word, count in word_counts.items():\n",
    "                for cls in self.classes:\n",
    "                    word_count_in_class = self.word_counts[cls].get(word, 0)\n",
    "                    total_words_in_class = sum(self.word_counts[cls].values())\n",
    "                    conditional_prob = (word_count_in_class + self.alpha) / (total_words_in_class + self.alpha * self.vocab_size)\n",
    "                    posterior_probs[cls] += count * np.log(conditional_prob)\n",
    "\n",
    "            predicted_class = max(posterior_probs, key=posterior_probs.get)\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "accuracies = []\n",
    "\n",
    "nb_clf = NaiveBayes(alpha=0.5, stemming=True)\n",
    "nb_clf.fit(X_train, y_train)\n",
    "y_pred = nb_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "for train_index, test_index in kf.split(X['abstract']):\n",
    "    X_train, X_test = X['abstract'][train_index], X['abstract'][test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    nb_clf.fit(X_train, y_train)\n",
    "    y_pred = nb_clf.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(\"5-fold cross-validation list of accuracies:\",accuracies)\n",
    "print(f\"5-fold cross-validation mean accuracy: {np.mean(accuracies):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to tst.csv predictions for kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, alpha=1.0, stemming=False):\n",
    "        self.word_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.class_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.class_priors = {}\n",
    "        self.alpha = alpha\n",
    "        self.stemming = stemming\n",
    "        if self.stemming:\n",
    "            self.stemmer = PorterStemmer()\n",
    "            \n",
    "            \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"alpha\": self.alpha,\n",
    "            \"stemming\": self.stemming,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        words = text.split()\n",
    "        processed_words = []\n",
    "        skip_next = False\n",
    "        skip_next_next = False\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if skip_next:\n",
    "                skip_next = False\n",
    "                if skip_next_next:\n",
    "                    skip_next = True\n",
    "                    skip_next_next = False\n",
    "                continue\n",
    "\n",
    "            if word == \"homo\" and i + 1 < len(words) and words[i + 1] == \"sapiens\":\n",
    "                processed_words.append(\"homo-sapiens\")\n",
    "                skip_next = True\n",
    "                continue\n",
    "\n",
    "            if word == \"escherichia\" and i + 1 < len(words) and words[i + 1] == \"coli\":\n",
    "                processed_words.append(\"escherichia-coli\")\n",
    "                skip_next = True\n",
    "                continue\n",
    "\n",
    "            if word == \"human\" and i + 2 < len(words) and words[i + 1] == \"immunodeficiency\" and words[i + 2] == \"virus\":\n",
    "                processed_words.append(\"human-immunodeficiency-virus\")\n",
    "                skip_next = True\n",
    "                skip_next_next = True\n",
    "                continue\n",
    "\n",
    "            if self.stemming:\n",
    "                word = self.stemmer.stem(word)\n",
    "\n",
    "            if word not in ENGLISH_STOP_WORDS:\n",
    "                processed_words.append(word)\n",
    "\n",
    "        return \" \".join(processed_words)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.vocab = set()\n",
    "\n",
    "        for cls in self.classes:\n",
    "            self.class_counts[cls] = sum(1 for label in y if label == cls)\n",
    "\n",
    "        for abstract, label in zip(X, y):\n",
    "            abstract = self.preprocess(abstract)\n",
    "            for word in set(abstract.split()):\n",
    "                self.word_counts[label][word] += 1\n",
    "                self.vocab.add(word)\n",
    "\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        total_docs = len(X)\n",
    "        self.class_priors = {cls: self.class_counts[cls] / total_docs for cls in self.classes}\n",
    "\n",
    "    def predict(self, documents):\n",
    "        predictions = []\n",
    "        for document in documents:\n",
    "            document = self.preprocess(document)\n",
    "            posterior_probs = {cls: np.log(self.class_priors[cls]) for cls in self.classes}\n",
    "            document_words = document.split()\n",
    "            word_counts = {word: document_words.count(word) for word in set(document_words)}\n",
    "\n",
    "            for word, count in word_counts.items():\n",
    "                for cls in self.classes:\n",
    "                    word_count_in_class = self.word_counts[cls].get(word, 0)\n",
    "                    total_words_in_class = sum(self.word_counts[cls].values())\n",
    "                    conditional_prob = (word_count_in_class + self.alpha) / (total_words_in_class + self.alpha * self.vocab_size)\n",
    "                    posterior_probs[cls] += count * np.log(conditional_prob)\n",
    "\n",
    "            predicted_class = max(posterior_probs, key=posterior_probs.get)\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "nb_clf = NaiveBayes(alpha=0.5, stemming=True)\n",
    "nb_clf.fit(X_train, y_train)\n",
    "\n",
    "data = pd.read_csv('tst.csv')\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "matrix = vectorizer.fit_transform(data.abstract)\n",
    "X = data.drop('id', axis=1)\n",
    "\n",
    "y_pred = nb_clf.predict(X['abstract'])\n",
    "\n",
    "df_predictions = pd.DataFrame({'id': range(1, len(y_pred) + 1), 'class': y_pred})\n",
    "df_predictions.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain and motivate the chosen representation & data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Representation\\\n",
    "The text data is represented as a bag-of-words, where each document is treated as an unordered collection of words. This allows the Naive Bayes classifier to capture the number of words in each document which can be informative for classification, while ignoring the word order and grammar. This representation is simple, efficient, and has been widely used in text classification tasks.\n",
    "\n",
    "- Data Preprocessing\n",
    "1. Tokenization:\\\n",
    "The preprocess function splits the text into individual words, which are the basic units for the bag-of-words representation.\n",
    "2. Stop Word Removal:\\\n",
    "Common English stop words (e.g., \"the\", \"a\", \"is\") are removed using the ENGLISH_STOP_WORDS set from the sklearn.feature_extraction.text module. Stop words typically have little effect for classification tasks and can be removed. This helps reduce the feature space and computational complexity, while potentially improving the classifier's performance by focusing on more relevant words.\n",
    "3. Stemming:\\\n",
    "The code provides an option to apply stemming using the PorterStemmer from the nltk.stem module. Stemming reduces words to their base or root form (e.g., \"running\" and \"ran\" become \"run\"). This can help in handling different word forms and potentially improve the classifier's performance by grouping related words together. However, it can also introduce some noise or ambiguity, so it's provided as an optional step.\n",
    "4. Handling Specific Phrases:\\\n",
    "The preprocess function includes special handling for certain phrases, such as \"homo sapiens\", \"escherichia coli\", and \"human immunodeficiency virus\". Treating certain phrases as single tokens can help preserve their meaning and avoid splitting them into separate words, which could potentially lead to a loss of context or meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain the idea behind the model improvements and their implementation (including the implementation of the standard Naive Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain the evaluation procedure (e.g., cross-validation or training/validation split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "include and explain the training/validation results for the standard and improved Naive Bayes model. You can summarize results using tables (or plots), but all results have to be explained descriptively as well. be written in plain English and should not be longer than two A4 pages (export the notebook as pdf to see if the report section fits in two pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Naive Bayes model\n",
    "- Accuracy: 0.93 - trained on 90% of the dataset\\\n",
    "5-fold cross-validation list of accuracies: [0.96, 0.9875, 0.9875, 0.99375, 0.99625]\\\n",
    "5-fold cross-validation mean accuracy: 0.98\n",
    "\n",
    "#### Improved Naive Bayes model\n",
    "- Accuracy: 0.97 - trained on 99% of the dataset\\\n",
    "5-fold cross-validation list of accuracies: [0.985, 0.99625, 0.99625, 0.99375, 0.9975]\\\n",
    "5-fold cross-validation mean accuracy: 0.99\n",
    "\n",
    "- Accuracy: 0.95 - trained on 90% of the dataset\\\n",
    "5-fold cross-validation list of accuracies: [0.965, 0.99625, 0.99625, 0.995, 0.9975]\\\n",
    "5-fold cross-validation mean accuracy: 0.99"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
