{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn\n",
    "!pip3 install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                           abstract\n",
      "0        1  the 4 202 353 bp genome of the alkaliphilic ba...\n",
      "1        2  the complete 1751377-bp sequence of the genome...\n",
      "2        3  in 1992 we started assembling an ordered libra...\n",
      "3        4  the aim of this study is to measure human mito...\n",
      "4        5  the amino acid sequence of the spirulina maxim...\n",
      "...    ...                                                ...\n",
      "3995  3996  we have isolated and characterized two diureti...\n",
      "3996  3997  myotonias are muscle diseases in which the fun...\n",
      "3997  3998  cysteine synthase o-acetylserine sulfhydrylase...\n",
      "3998  3999  a region of 25 nucleotides is highly conserved...\n",
      "3999  4000  thermoanaerobacter tengcongensis is a rod-shap...\n",
      "\n",
      "[4000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = pd.read_csv('trg.csv')\n",
    "vectorizer = CountVectorizer()\n",
    "matrix = vectorizer.fit_transform(data.abstract)\n",
    "\n",
    "data = data.rename(columns={'class': 'class_label'})\n",
    "X = data.drop('class_label', axis=1)\n",
    "y = data['class_label']\n",
    "data  = pd.DataFrame(data= matrix.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "print(X)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['abstract'], y, test_size=0.1, random_state=42)\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.word_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.class_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.class_priors = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.vocab = set()\n",
    "        for cls in self.classes:\n",
    "            self.class_counts[cls] = sum(1 for label in y if label == cls)\n",
    "        for abstract, label in zip(X, y):\n",
    "            for word in set(abstract.split()):\n",
    "                self.word_counts[label][word] += 1\n",
    "                self.vocab.add(word)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        total_docs = len(X)\n",
    "        self.class_priors = {cls: self.class_counts[cls] / total_docs for cls in self.classes}\n",
    "\n",
    "    def predict(self, documents):\n",
    "        predictions = []\n",
    "        for document in documents:\n",
    "            posterior_probs = {cls: np.log(self.class_priors[cls]) for cls in self.classes}\n",
    "            document_words = document.split()\n",
    "            word_counts = {word: document_words.count(word) for word in set(document_words)}\n",
    "            for word, count in word_counts.items():\n",
    "                for cls in self.classes:\n",
    "                    word_count_in_class = self.word_counts[cls].get(word, 0)\n",
    "                    total_words_in_class = sum(self.word_counts[cls].values())\n",
    "                    conditional_prob = (word_count_in_class + 1) / (total_words_in_class + self.vocab_size)\n",
    "                    posterior_probs[cls] += count * np.log(conditional_prob)\n",
    "            predicted_class = max(posterior_probs, key=posterior_probs.get)\n",
    "            predictions.append(predicted_class)\n",
    "        return predictions\n",
    "\n",
    "nb_clf = NaiveBayes()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "y_pred = nb_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['abstract'], y, test_size=0.1, random_state=42)\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.word_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.class_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.class_priors = {}\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        words = text.split()\n",
    "        processed_words = []\n",
    "        skip_next = False\n",
    "        skip_next_next = False\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if skip_next:\n",
    "                skip_next = False\n",
    "                if skip_next_next:\n",
    "                    skip_next = True\n",
    "                    skip_next_next = False\n",
    "                continue\n",
    "\n",
    "            if word == \"homo\" and i + 1 < len(words) and words[i + 1] == \"sapiens\":\n",
    "                processed_words.append(\"homo-sapiens\")\n",
    "                skip_next = True\n",
    "                continue\n",
    "\n",
    "            if word == \"escherichia\" and i + 1 < len(words) and words[i + 1] == \"coli\":\n",
    "                processed_words.append(\"escherichia-coli\")\n",
    "                skip_next = True\n",
    "                continue\n",
    "\n",
    "            if word == \"human\" and i + 2 < len(words) and words[i + 1] == \"immunodeficiency\" and words[i + 2] == \"virus\":\n",
    "                processed_words.append(\"human-immunodeficiency-virus\")\n",
    "                skip_next = True\n",
    "                skip_next_next = True\n",
    "                continue\n",
    "\n",
    "            processed_words.append(word)\n",
    "\n",
    "        return \" \".join(processed_words)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.vocab = set()\n",
    "\n",
    "        for cls in self.classes:\n",
    "            self.class_counts[cls] = sum(1 for label in y if label == cls)\n",
    "\n",
    "        for abstract, label in zip(X, y):\n",
    "            abstract = self.preprocess(abstract)\n",
    "            for word in set(abstract.split()):\n",
    "                self.word_counts[label][word] += 1\n",
    "                self.vocab.add(word)\n",
    "\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        total_docs = len(X)\n",
    "        self.class_priors = {cls: self.class_counts[cls] / total_docs for cls in self.classes}\n",
    "\n",
    "    def predict(self, documents):\n",
    "        predictions = []\n",
    "        for document in documents:\n",
    "            document = self.preprocess(document)\n",
    "            posterior_probs = {cls: np.log(self.class_priors[cls]) for cls in self.classes}\n",
    "\n",
    "            document_words = document.split()\n",
    "            word_counts = {word: document_words.count(word) for word in set(document_words)}\n",
    "\n",
    "            for word, count in word_counts.items():\n",
    "                for cls in self.classes:\n",
    "                    word_count_in_class = self.word_counts[cls].get(word, 0)\n",
    "                    total_words_in_class = sum(self.word_counts[cls].values())\n",
    "                    conditional_prob = (word_count_in_class + 1) / (total_words_in_class + self.vocab_size)\n",
    "                    posterior_probs[cls] += count * np.log(conditional_prob)\n",
    "\n",
    "            predicted_class = max(posterior_probs, key=posterior_probs.get)\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "nb_clf = NaiveBayes()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "y_pred = nb_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used on tst.csv to for kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              abstract\n",
      "0    in a previous work all three components of com...\n",
      "1    we compared morphology of two geographically c...\n",
      "2    factor xiii mr 320000 is a blood coagulation f...\n",
      "3    we report the characterisation of a human gene...\n",
      "4    fat tissue plays a critical role in the regula...\n",
      "..                                                 ...\n",
      "995  the molecular chaperonins such as groel are no...\n",
      "996  the cdna sequence of the flavoprotein subunit ...\n",
      "997  the higher plant arabidopsis thaliana arabidop...\n",
      "998  the hyperthermophilic euryarchaeon pyrococcus ...\n",
      "999  the complete mitochondrial dna mtdna molecule ...\n",
      "\n",
      "[1000 rows x 1 columns]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NaiveBayes' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(X)\n\u001b[0;32m     13\u001b[0m nb_clf \u001b[38;5;241m=\u001b[39m NaiveBayes()\n\u001b[1;32m---> 14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mnb_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m df_predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(y_pred) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: y_pred})\n\u001b[0;32m     17\u001b[0m df_predictions\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[118], line 70\u001b[0m, in \u001b[0;36mNaiveBayes.predict\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m     69\u001b[0m     document \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(document)\n\u001b[1;32m---> 70\u001b[0m     posterior_probs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mcls\u001b[39m: np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_priors[\u001b[38;5;28mcls\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m}\n\u001b[0;32m     72\u001b[0m     document_words \u001b[38;5;241m=\u001b[39m document\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     73\u001b[0m     word_counts \u001b[38;5;241m=\u001b[39m {word: document_words\u001b[38;5;241m.\u001b[39mcount(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(document_words)}\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NaiveBayes' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('tst.csv')\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "matrix = vectorizer.fit_transform(data.abstract)\n",
    "X = data.drop('id', axis=1)\n",
    "print(X)\n",
    "\n",
    "nb_clf = NaiveBayes()\n",
    "y_pred = nb_clf.predict(X['abstract'])\n",
    "\n",
    "df_predictions = pd.DataFrame({'id': range(1, len(y_pred) + 1), 'class': y_pred})\n",
    "df_predictions.to_csv('predictions.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
